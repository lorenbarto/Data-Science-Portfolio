{\rtf1\ansi\ansicpg1252\cocoartf1671\cocoasubrtf200
{\fonttbl\f0\fnil\fcharset0 Monaco;\f1\fnil\fcharset0 Menlo-Regular;}
{\colortbl;\red255\green255\blue255;\red107\green0\blue109;\red255\green255\blue255;\red10\green86\blue216;
\red3\green53\blue197;\red1\green30\blue103;\red247\green247\blue247;\red38\green38\blue38;\red252\green106\blue8;
}
{\*\expandedcolortbl;;\cssrgb\c50196\c0\c50196;\cssrgb\c100000\c100000\c100000;\cssrgb\c0\c43529\c87843;
\cssrgb\c0\c30588\c81569;\cssrgb\c0\c17647\c47843;\cssrgb\c97647\c97647\c97647;\cssrgb\c20000\c20000\c20000;\cssrgb\c100000\c50196\c0;
}
\margl1440\margr1440\vieww51000\viewh20820\viewkind0
\deftab720
\pard\pardeftab720\partightenfactor0

\f0\fs24 \cf2 \cb3 \expnd0\expndtw0\kerning0
from\cf4  \cf5 pyspark \cf2 import\cf4  \cf5 SparkContext\cf0 \cb1 \
\cf6 \cb7 sc\cf4 =\cf5 SparkContext\cf8 ()\cf0 \
\cf9 \cb3 # sc is an existing SparkContext.\cf0 \cb1 \
\cf2 \cb7 from\cf4  \cf6 pyspark\cf8 .\cf5 sql \cf2 import\cf4  \cf6 SQLContext\cf8 ,\cf4  \cf5 Row\cf0 \
\cf6 \cb3 sqlContext\cf4  = \cf5 SQLContext\cf8 (\cf6 sc\cf8 )\cf0 \cb1 \
\cf2 \cb7 import\cf4  \cf2 urllib\cf0 \
\cf2 \cb3 from\cf4  \cf2 datetime\cf4  \cf2 import\cf4  \cf6 timedelta\cf8 ,\cf4  \cf5 date
\f1\fs22 \cf0 \cb1 \kerning1\expnd0\expndtw0 \
\pard\pardeftab720\ri0\sb240\sa180\qj\partightenfactor0
\cf0 val sqlContext = new org.apache.spark.sql.SQLContext(sc)\
import sqlContext.implicits._\
sqlContext.sql("show tables").show()\
\pard\tx560\tx1120\tx1680\tx2240\tx2800\tx3360\tx3920\tx4480\tx5040\tx5600\tx6160\tx6720\pardeftab720\ri0\partightenfactor0
\cf0 dataframe_mysql = SparkSession.read.format("jdbc").options(url="jdbc:mysql://nightingaledb:3306/pyspark", driver = "com.mysql.jdbc.Driver", dbtable = "mortality_data", user="lati", password="LATtyb@123").load().take(10)\
}